{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Plasticity_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbM2ROhG8iHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqgvM9QgC-WT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nr_classes = 10177 #unique identities\n",
        "img_size = (218, 218) #taken as reference for conv. operations\n",
        "flattened_kernel = 12 *12 * 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ckh-pIJ9Ocn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DEFINE MODEL\n",
        "\n",
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    #initialize plasticity parameters\n",
        "    self.alpha = torch.nn.Parameter(torch.rand(12 * 12 * 64,\n",
        "                                                nr_classes),\n",
        "                                    requires_grad = True)\n",
        "    self.hebb_trace = torch.nn.Parameter(torch.zeros(12 * 12 * 64,\n",
        "                                                     nr_classes),\n",
        "                                         requires_grad = False)\n",
        "    self.eta = torch.nn.Parameter(1, requires_grad = True) #all conn. share same value of eta \n",
        "\n",
        "    #initialize \"fixed\" parameters, weights\n",
        "    self.w = torch.nn.Parameter(torch.rand(12 * 12 * 64), nr_classes)\n",
        "\n",
        "    #initialize layers\n",
        "    self.cv1 = torch.nn.Conv2d(in_channels = 2, out_channels = 64\n",
        "                                 kernel_size = 3, stride = 2)\n",
        "    self.cv2 = torch.nn.Conv2d(64, 64, 3, stride = 2)\n",
        "    self.cv2 = torch.nn.Conv2d(64, 64, 3, stride = 2)\n",
        "    self.cv4 = torch.nn.Conv2d(64, 64, 3, stride = 2)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    #Convolutions,activation function (and pooling) to be discussed\n",
        "    x = F.relu(self.cv1(x))\n",
        "    x = F.relu(self.cv2(x_i))\n",
        "    x = F.relu(self.cv3(x))\n",
        "    x = F.relu(self.cv4(x))\n",
        "    #Flatten\n",
        "    x_i = x.view(-1, 12 * 12 * 64) #x_i.shape: [batch_size, flattened_kernel]\n",
        "    #Plastic layer\n",
        "    x_j = x_i.mm(self.w + torch.mul(self.alpha, self.hebb_trace)) #x_j.shape: [batch_size, nr_classes]\n",
        "    x = F.softmax(x_j)\n",
        "    hebb_trace = self.update_hebb_trace(\n",
        "        self.eta, x_i, x_j, hebb_trace)\n",
        "    return x, hebb_trace\n",
        "\n",
        "  def update_hebb_trace(self, eta, x_i, x_j, hebb_trace):\n",
        "    return self.eta * torch.mm(x_i.T, x_j) + hebb_trace * (1 - self.eta) #returns Tensor.shape: [flattened_kernel, nr_classes]\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}